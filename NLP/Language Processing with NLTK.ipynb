{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing NLTK\n",
    "if the nltk library dose not exist on your system currently install it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Nltk's download features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important terms in NLP\n",
    "- tokenizers : word tolkenizers.... sentence tokenizers - seperators \n",
    "- corporas- : medical journals / speeches / english language\n",
    "- lexicon - : words and their means - the terminology for somthing like investors or doctors \n",
    "\n",
    "These are the words you will most commonly hear upon entering the Natural Language Processing (NLP) space, but there are many more that we will be covering in time. With that, let's show an example of how one might actually tokenize something into tokens with the NLTK module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "print(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Process \n",
    "\n",
    "At first, you may think tokenizing by things like words or sentences is a rather trivial enterprise. For many sentences it can be. The first step would be likely doing a simple .split('. '), or splitting by period followed by a space. Then maybe you would bring in some regular expressions to split by period, space, and then a capital letter. The problem is that things like Mr. Smith would cause you trouble, and many other things. Splitting by word is also a challenge, especially when considering things like concatenations like we and are to we're. NLTK is going to go ahead and just save you a ton of time with this seemingly simple, yet very complex, operation.\n",
    "\n",
    "The above code will output the sentences, split up into a list of sentences, which you can do things like iterate through with a for loop.\n",
    "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n",
    "\n",
    "So there, we have created tokens, which are sentences. Let's tokenize by word instead this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "# here lets tokenize a sentence by word \n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      ",\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n",
      ".\n",
      "You\n",
      "should\n",
      "n't\n",
      "eat\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(EXAMPLE_TEXT):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeling Stop Words \n",
    "Immediately, we can recognize ourselves that some words carry more meaning than other words. We can also see that some words are just plain useless, and are filler words. We use them in the English language, for example, to sort of \"fluff\" up the sentence so it is not so strange sounding. An example of one of the most common, unofficial, useless words is the phrase \"umm.\" People stuff in \"umm\" frequently, some more than others. This word means nothing, unless of course we're searching for someone who is maybe lacking confidence, is confused, or hasn't practiced much speaking. We all do it, you can hear me saying \"umm\" or \"uhh\" in the videos plenty of ...uh ... times. For most analysis, these words are useless.\n",
    "\n",
    "We would not want these words taking up space in our database, or taking up valuable processing time. As such, we call these words \"stop words\" because they are useless, and we wish to do nothing with them. Another version of the term \"stop words\" can be more literal: Words we stop on.\n",
    "\n",
    "For example, you may wish to completely cease analysis if you detect words that are commonly used sarcastically, and stop immediately. Sarcastic words, or phrases are going to vary by lexicon and corpus. For now, we'll be considering stop words as words that just contain no meaning, and we want to remove them.\n",
    "\n",
    "You can do this easily, by storing a list of words that you consider to be stop words. NLTK starts you off with a bunch of words that they consider to be stop words, you can access it via the NLTK corpus with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#first lets bring in some stuff from corpus \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'just', \"don't\", 'this', 'being', 'up', 'doesn', \"weren't\", \"shan't\", 'where', 'be', \"hadn't\", 'there', 'needn', 'hasn', \"wouldn't\", 'aren', 'from', 'ain', 'against', 'which', 'about', 'will', 'shouldn', 'our', 'should', 'yourselves', 'ma', 'his', 'after', 'very', 'now', 'is', 'having', 'her', \"you'd\", 'with', 'no', 'wouldn', 'own', 'if', 'isn', 'as', 'him', 'themselves', 'an', 'below', 'why', 'few', 'off', 'do', \"it's\", 'your', 'out', 'further', 'above', 'my', 'these', 'over', \"shouldn't\", \"you're\", 'you', \"isn't\", 'by', 'am', 'yourself', 'who', 'while', 'down', 'all', 'only', 'himself', 'such', 'because', 'other', 'between', 'was', 'wasn', 'hers', 'on', 'how', 'haven', \"won't\", \"doesn't\", \"that'll\", 'again', 'didn', 'same', 'won', 'some', 'both', 'theirs', 'to', \"wasn't\", 'too', 'myself', 'their', 's', 'i', \"haven't\", \"you'll\", 'been', 'not', 'when', 'y', 'ours', 'each', 'm', 'so', 'me', 't', 'she', 'does', 're', 'have', \"needn't\", \"she's\", 'it', 'has', 'at', 'under', 'then', 've', 'until', 'those', 'that', 'are', \"mustn't\", 'but', \"aren't\", 'shan', 'during', 'its', 'had', 'whom', 'of', \"didn't\", 'can', \"should've\", 'before', \"hasn't\", 'and', 'here', 'more', 'weren', 'any', 'couldn', 'or', 'herself', 'once', 'through', 'o', \"you've\", 'hadn', 'were', 'mightn', 'than', 'mustn', 'yours', 'did', 'nor', 'in', 'them', 'doing', 'we', 'itself', 'they', 'into', 'for', 'd', 'what', \"couldn't\", 'don', 'most', 'he', 'a', 'the', 'ourselves', 'll', \"mightn't\"}\n"
     ]
    }
   ],
   "source": [
    "# set in an example sentence \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#lets examine what nltk uses as stop words \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'filtration',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets use it now on our example sentence \n",
    "words = word_tokenize(example_sent)\n",
    "\n",
    "filterd_sentence =[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filterd_sentence.append(w)\n",
    "        \n",
    "# you could also code it like this         \n",
    "#filterd_sentence = [w for w in words if not w in stop_words]\n",
    "        \n",
    "filterd_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Words \n",
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "\n",
    "Consider:\n",
    "\n",
    "I was taking a ride in the car.\n",
    "I was riding in the car.\n",
    "This sentence means the same thing. in the car is the same. I was is the same. the ing denotes a clear past-tense in both cases, so is it truly necessary to differentiate between ride and riding, in the case of just trying to figure out the meaning of what this past-tense activity was?\n",
    "\n",
    "No, not really.\n",
    "\n",
    "This is just one minor example, but imagine every word in the English language, every possible tense and affix you can put on a word. Having individual dictionary entries per version would be highly redundant and inefficient, especially since, once we convert to numbers, the \"value\" is going to be identical.\n",
    "\n",
    "One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979.\n",
    "\n",
    "First, we're going to grab and define our stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PorterStemmer>\n"
     ]
    }
   ],
   "source": [
    "# grab our Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "#lets test out setemming on some examples\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for i in example_words:\n",
    "    print(ps.stem(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# now lets use the stemmer on a actual sentence \n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
